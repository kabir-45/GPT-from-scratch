{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "e2cf6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import re\n",
    "# class Tokenizer:\n",
    "    \n",
    "#     def __init__(self, vocab):\n",
    "#         self.str_to_int = vocab # vocab is a mapping of tokens to token IDs\n",
    "#         self.int_to_str = {i:s for i,s in vocab.items()}\n",
    "    \n",
    "#     def encode(self, text): # convert test to token Ids\n",
    "#         processed = re.split(r'([:,.?_=+()-!\";\\'@#$%&*]|--|\\s)', text)\n",
    "#         processed = [item.strip() for item in processed if item.strip()] # removing the blank spaces\n",
    "#         processed = [\n",
    "#             item if item in self.str_to_int\n",
    "#             else \"<unk>\" for item in preprocessed\n",
    "#         ]\n",
    "#         ids = [self.str__to_int[s] for s in processed]\n",
    "#         return ids\n",
    "    \n",
    "#     def decode(self, ids): # convert token IDs to text\n",
    "#         text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "#         text = re.sub(r'\\s+([,.?();\":!])', r'\\1',text)\n",
    "#         return text \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "a11192fe-b48b-4228-b9fa-d747705a4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\siddh\\anaconda_for_py\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\siddh\\anaconda_for_py\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\siddh\\anaconda_for_py\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\siddh\\anaconda_for_py\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\siddh\\anaconda_for_py\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\siddh\\anaconda_for_py\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\siddh\\anaconda_for_py\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "97da18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte pair tokenization\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "24bd5bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alice in wonderland book as dataset \n",
    "import requests\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/7256/pg7256-images.html\"  # stable Alice txt URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Write to file\n",
    "with open(\"alice.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "# Now read from file\n",
    "with open(\"alice.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "a1c81095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Siddh\\anaconda_for_py\\Lib\\site-packages\\torch\\__init__.py\n",
      "2.7.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__file__)        # Path to actual torch package\n",
    "print(torch.__version__)     # Should show version string\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "54cfa70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataprep(Dataset):\n",
    "    \n",
    "    def __init__(self, text,tokenizer, max_length, stride):\n",
    "        self.input_tensor = []\n",
    "        self.target_tensor = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_window = token_ids[i:i+max_length]\n",
    "            target_window = token_ids[i+stride:i+max_length+stride]\n",
    "            self.input_tensor.append(torch.tensor(input_window))\n",
    "            self.target_tensor.append(torch.tensor(target_window))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensor)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_tensor[idx] , self.target_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "d9768f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(text, batch_size=4, max_length=256, stride=128, drop_last=True, shuffle=False):\n",
    "    \n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        dataset = Dataprep(text, tokenizer, max_length, stride)\n",
    "        \n",
    "        dataloader = DataLoader(   \n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            drop_last=drop_last,\n",
    "            shuffle=shuffle\n",
    "            )\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "7658e597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11071\n",
      "37345\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.encode(text)))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "a64ccd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the inputs and targets tensor of dims 8x4\n",
    "dataloader = create_dataloader(text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs , targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "afa74158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vector embeddings\n",
    "vocab_size = 50257 # gpt2 had 50257 tokens\n",
    "embed_dim = 256\n",
    "vec_embed_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "vector_embeddings = vec_embed_layer(inputs)\n",
    "\n",
    "# positional encodings\n",
    "max_length = 4\n",
    "pos_encoding_layer = torch.nn.Embedding(max_length, embed_dim)\n",
    "pos_encodings = pos_encoding_layer(torch.arange(max_length))\n",
    "\n",
    "# input embeddings \n",
    "input_embed = vector_embeddings + pos_encodings\n",
    "input_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "5c52a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dout, qkv_bias=False):\n",
    "        super().init()\n",
    "        self.Wq = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "        self.Wk = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "        self.Wv = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        attention_scores = Q @ K.T\n",
    "        atten_weights = torch.softmax(attention_scores / K.shape[-1]**0.5, dim = -1)\n",
    "        context_vector = atten_weights @ V\n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "661b960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CausalAttention(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, din, dout, context_length, dropout_rate, qkv_bias=False):\n",
    "#         super.init()\n",
    "#         self.Wq = torch.nn.Linear(din, dout, bias=qkv_bias) # dout is the user defiend but din should strictly be equal to dim = embed_size for matrix mul\n",
    "#         self.Wk = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "#         self.Wv = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "#         self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "#         self.register_buffer(torch.triu(torch.ones('mask',context_length, context_length), diagonal=1))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         batch, no_tokens, din = x.shape\n",
    "#         Q = self.Wq(x)\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "        \n",
    "#         attention_scores = Q @ K.transpose(1,2)\n",
    "#         masked = attention_scores.masked_fill_(mask.bool()[:no_tokens, no_tokens], -torch.inf)\n",
    "#         atten_weights = torch.softmax(masked / K.shape[-1] ** 0.5, dim=-1)\n",
    "#         atten_weights = self.dropout(atten_weights)\n",
    "#         context_vector = atten_weights @ V\n",
    "        \n",
    "#         return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "b0948ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, din, dout, context_length, num_heads, dropout_rate):\n",
    "#         super().__init__()\n",
    "#         self.head = torch.Modulelist(\n",
    "#             [CausalAttention(din, dout, context_length, dropout_rate, qkv_bias) for _ in range(num_heads)]\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return torch.cat( [head[x] for head in self.head] , dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "7488e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedMultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dout, context_length, num_heads, dropout_rate, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dout = dout\n",
    "        self.head_dim  = dout // num_heads\n",
    "        self.Wq = torch.nn.Linear(din, dout, bias=qkv_bias) # dout is the user defiend but din should strictly be equal to dim = embed_size for matrix mul\n",
    "        self.Wk = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "        self.Wv = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "        self.out_proj = torch.nn.Linear(dout, dout)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, n_tokens, din = x.shape\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        Q = Q.view(batch, n_tokens, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch, n_tokens, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch, n_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        Q = Q.transpose(1,2)\n",
    "        V = V.transpose(1,2)\n",
    "        K = K.transpose(1,2)\n",
    "        \n",
    "        attention_scores = Q @ K.transpose(2,3)\n",
    "        masked = attention_scores.masked_fill_(self.mask.bool()[:n_tokens, :n_tokens], -torch.inf)\n",
    "        atten_weights = torch.softmax(masked/self.head_dim ** 0.5, dim=-1)\n",
    "        atten_weights = self.dropout(atten_weights)\n",
    "        context_vector = (atten_weights @ V).transpose(1,2)\n",
    "        context_vector = context_vector.contiguous().view(batch, n_tokens, self.dout)\n",
    "        context_vector =  self.out_proj(context_vector)\n",
    "        \n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "88596c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\":256,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_layer\":12,\n",
    "    \"n_head\":12,\n",
    "    \"dropout\":0.1,\n",
    "    \"qkv_bias\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "51c9c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.espilon = 1e-7\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        var = x.var(axis=-1, keepdims=True, unbiased=True)\n",
    "        norm_x = (x - mean)/torch.sqrt(var + self.espilon )\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        gelu = 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2/torch.pi))) * (x + 0.0447 * torch.pow(x,3)))\n",
    "        return gelu\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "6262fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedMultiheadAttention(\n",
    "            din = cfg[\"emb_dim\"], \n",
    "            dout = cfg[\"emb_dim\"], \n",
    "            context_length = cfg[\"context_length\"], \n",
    "            num_heads = cfg[\"n_head\"], \n",
    "            dropout_rate = cfg[\"dropout\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.norm1 = LayerNorm(embed_dim = cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(embed_dim = cfg[\"emb_dim\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.drop = nn.Dropout(cfg[\"dropout\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x) # pre normalization results in better reults than post normalization\n",
    "        x = self.attention(x)\n",
    "        x = self.drop(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "746534e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_layer = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"dropout\"])\n",
    "        \n",
    "        self.tf_blocks = nn.Sequential(\n",
    "            *[Transformer(cfg) for _ in range(cfg[\"n_layer\"])])\n",
    "        \n",
    "        self.norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_ids):\n",
    "        batch, seq_len = in_ids.shape\n",
    "        token_embed = self.embed_layer(in_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=in_ids.device).unsqueeze(0).expand(batch, -1)\n",
    "        pos_encoding = self.pos_layer(pos_ids)\n",
    "        x = token_embed + pos_encoding\n",
    "        x = self.dropout(x)\n",
    "        x = self.tf_blocks(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "8881d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_tokens(idx, model, max_new_tokens, context_length, temperature=1.0, top_k=50, top_p=0.9):\n",
    "    idx = idx.to(dtype=torch.long, device=model.pos_layer.weight.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_length:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Top-k filtering\n",
    "        if top_k is not None:\n",
    "            top_k = min(top_k, probs.size(-1))  # Safety\n",
    "            top_k_vals, top_k_indices = torch.topk(probs, top_k)\n",
    "            probs_filtered = torch.zeros_like(probs).scatter_(1, top_k_indices, top_k_vals)\n",
    "            probs = probs_filtered / probs_filtered.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # Nucleus (top-p) filtering\n",
    "        if top_p is not None:\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            cum_probs = sorted_probs.cumsum(dim=-1)\n",
    "\n",
    "            # Create mask for tokens to keep\n",
    "            keep_mask = cum_probs <= top_p\n",
    "            # Always keep at least 1 token\n",
    "            keep_mask[..., 0] = 1\n",
    "\n",
    "            filtered_probs = torch.zeros_like(probs)\n",
    "            filtered_probs.scatter_(1, sorted_indices, keep_mask.float() * sorted_probs)\n",
    "            probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "e40e3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total model parameters: 162,419,712\n"
     ]
    }
   ],
   "source": [
    "model = GPT(cfg)\n",
    "model_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"total model parameters: {model_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "41f2a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropyloss(input_batch, target_batch, model, device):\n",
    "    input_batch , target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss(dataloader, device, model, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return float('nan')\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else :\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            total_loss += crossentropyloss(input_batch, target_batch, model, device).item()\n",
    "        else :\n",
    "            break\n",
    "    return  total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "7c15685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "train_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n",
      "val_loader:(torch.Size([4, 256]), torch.Size([4, 256]))\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.85\n",
    "data_split  = int(train_split * len(text))\n",
    "train_data = text[:data_split]\n",
    "val_data = text[data_split:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader(train_data, batch_size=4, max_length=cfg[\"context_length\"], stride=cfg[\"context_length\"], drop_last=True, shuffle=False)\n",
    "val_loader = create_dataloader(val_data, batch_size=4, max_length=cfg[\"context_length\"], stride=cfg[\"context_length\"], drop_last=True, shuffle=False)\n",
    "\n",
    "for x,y in train_loader:\n",
    "    print(f\"train_loader:{x.shape, y.shape}\")\n",
    "    \n",
    "for x,y in val_loader:\n",
    "    print(f\"val_loader:{x.shape, y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "26d3deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.992869483100044\n",
      "Validation loss: 11.012042045593262\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss(train_loader, device, model)\n",
    "    val_loss = calc_loss(val_loader, device,  model)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "d96e6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a56c500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, device, eval_iter, train_loader, val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss(train_loader, device, model, num_batches=eval_iter)\n",
    "        val_loss = calc_loss(val_loader, device, model, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "d92f491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_print_sample(model, device, tokenizer, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_layer.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_next_tokens(\n",
    "    model=model, \n",
    "    idx=encoded, \n",
    "    max_new_tokens=20, \n",
    "    context_length=context_size, \n",
    "    temperature=1.0, \n",
    "    top_k=50, \n",
    "    top_p=0.9\n",
    ")\n",
    "    decoded = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "d77a81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, eval_iter, eval_freq, num_epochs, start_context, optimizer,tokenizer):\n",
    "    \n",
    "    train_loss, val_loss, tokens_seen = [], [], []\n",
    "    tokens_seen1, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = crossentropyloss(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen1 += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0 :\n",
    "                train_loss1, val_loss1 = evaluate_model(model, device, eval_iter, train_loader, val_loader)\n",
    "                train_loss.append(train_loss1)\n",
    "                val_loss.append(val_loss1)\n",
    "                tokens_seen.append(tokens_seen1)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss[-1]:.3f}, Val loss {val_loss[-1]:.3f}\")\n",
    "\n",
    "                \n",
    "        generate_print_sample(model, device, tokenizer, start_context)\n",
    "    return train_loss, val_loss, tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "2d54185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 10.099, Val loss 10.863\n",
      "Ep 1 (Step 000005): Train loss 7.941, Val loss 8.550\n",
      "There was a        . >  >  the  .  \n",
      "Ep 2 (Step 000010): Train loss 6.721, Val loss 7.750\n",
      "Ep 2 (Step 000015): Train loss 5.974, Val loss 7.185\n",
      "There was a  with ofp., of  the of: p,-:.   and the\n",
      "Ep 3 (Step 000020): Train loss 5.836, Val loss 7.194\n",
      "Ep 3 (Step 000025): Train loss 5.740, Val loss 7.269\n",
      "There was a  on  � � the or� Gutenberg p  Gutenberg�   thep. \n",
      "Ep 4 (Step 000030): Train loss 5.710, Val loss 7.389\n",
      "Ep 4 (Step 000035): Train loss 5.720, Val loss 7.453\n",
      "There was a  the the.  a�   . p, with   >  you\n",
      "Ep 5 (Step 000040): Train loss 5.682, Val loss 7.527\n",
      "There was a .p. to>™<     for; of    the   of\n",
      "Training completed in 16.63 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# Uncomment the following code to calculate the execution time\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPT(cfg)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_loss, val_loss, tokens_seen = train_model(\n",
    "    model, train_loader, val_loader, device, eval_iter=5, eval_freq=5,\n",
    "    num_epochs=num_epochs, start_context=\"There was a \", optimizer=optimizer, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Note:\n",
    "# Uncomment the following code to show the execution time\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
