{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2cf6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import re\n",
    "# class Tokenizer:\n",
    "    \n",
    "#     def __init__(self, vocab):\n",
    "#         self.str_to_int = vocab # vocab is a mapping of tokens to token IDs\n",
    "#         self.int_to_str = {i:s for i,s in vocab.items()}\n",
    "    \n",
    "#     def encode(self, text): # convert test to token Ids\n",
    "#         processed = re.split(r'([:,.?_=+()-!\";\\'@#$%&*]|--|\\s)', text)\n",
    "#         processed = [item.strip() for item in processed if item.strip()] # removing the blank spaces\n",
    "#         processed = [\n",
    "#             item if item in self.str_to_int\n",
    "#             else \"<unk>\" for item in preprocessed\n",
    "#         ]\n",
    "#         ids = [self.str__to_int[s] for s in processed]\n",
    "#         return ids\n",
    "    \n",
    "#     def decode(self, ids): # convert token IDs to text\n",
    "#         text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "#         text = re.sub(r'\\s+([,.?();\":!])', r'\\1',text)\n",
    "#         return text \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11192fe-b48b-4228-b9fa-d747705a4c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\kabir ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\kabir ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\kabir ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kabir ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kabir ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kabir ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kabir ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97da18a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte pair tokenization\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24bd5bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved!\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(\"the-verdict.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "    print(\"File downloaded and saved!\")\n",
    "else:\n",
    "    print(\"Failed to download file:\", response.status_code)\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "print(text[:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1c81095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\__init__.py\n",
      "2.5.1\n",
      "True\n",
      "NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__file__)        # Path to actual torch package\n",
    "print(torch.__version__)     # Should show version string\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54cfa70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Dataprep(Dataset):\n",
    "    \n",
    "    def __init__(self, text,tokenizer, max_length, stride):\n",
    "        self.input_tensor = []\n",
    "        self.target_tensor = []\n",
    "        \n",
    "        token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        for i in range(0, len(token_ids) - max_length - stride + 1, stride):\n",
    "            input_window = token_ids[i:i + max_length]\n",
    "            target_window = token_ids[i + stride:i + max_length + stride]\n",
    "\n",
    "            if len(target_window) == max_length:\n",
    "                self.input_tensor.append(torch.tensor(input_window))\n",
    "                self.target_tensor.append(torch.tensor(target_window))\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_tensor)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_tensor[idx] , self.target_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9768f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(text, batch_size=4, max_length=256, stride=128, drop_last=True, shuffle=False):\n",
    "    \n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        dataset = Dataprep(text, tokenizer, max_length, stride)\n",
    "        \n",
    "        dataloader = DataLoader(   \n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            drop_last=drop_last,\n",
    "            shuffle=shuffle\n",
    "            )\n",
    "        return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7658e597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n",
      "20479\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer.encode(text)))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a64ccd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creating the inputs and targets tensor of dims 8x4\n",
    "# dataloader = create_dataloader(text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "# data_iter = iter(dataloader)\n",
    "# inputs , targets = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afa74158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # vector embeddings\n",
    "# vocab_size = 50257 # gpt2 had 50257 tokens\n",
    "# embed_dim = 256\n",
    "# vec_embed_layer = torch.nn.Embedding(vocab_size, embed_dim)\n",
    "# vector_embeddings = vec_embed_layer(inputs)\n",
    "\n",
    "# # positional encodings\n",
    "# max_length = 4\n",
    "# pos_encoding_layer = torch.nn.Embedding(max_length, embed_dim)\n",
    "# pos_encodings = pos_encoding_layer(torch.arange(max_length))\n",
    "\n",
    "# # input embeddings \n",
    "# input_embed = vector_embeddings + pos_encodings\n",
    "# input_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c52a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class self_attention(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, din, dout, qkv_bias=False):\n",
    "#         super().init()\n",
    "#         self.Wq = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "#         self.Wk = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "#         self.Wv = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         Q = self.Wq(x)\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "        \n",
    "#         attention_scores = Q @ K.T\n",
    "#         atten_weights = torch.softmax(attention_scores / K.shape[-1]**0.5, dim = -1)\n",
    "#         context_vector = atten_weights @ V\n",
    "#         return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "661b960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CausalAttention(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, din, dout, context_length, dropout_rate, qkv_bias=False):\n",
    "#         super.init()\n",
    "#         self.Wq = torch.nn.Linear(din, dout, bias=qkv_bias) # dout is the user defiend but din should strictly be equal to dim = embed_size for matrix mul\n",
    "#         self.Wk = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "#         self.Wv = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "#         self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "#         self.register_buffer(torch.triu(torch.ones('mask',context_length, context_length), diagonal=1))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         batch, no_tokens, din = x.shape\n",
    "#         Q = self.Wq(x)\n",
    "#         K = self.Wk(x)\n",
    "#         V = self.Wv(x)\n",
    "        \n",
    "#         attention_scores = Q @ K.transpose(1,2)\n",
    "#         masked = attention_scores.masked_fill_(mask.bool()[:no_tokens, no_tokens], -torch.inf)\n",
    "#         atten_weights = torch.softmax(masked / K.shape[-1] ** 0.5, dim=-1)\n",
    "#         atten_weights = self.dropout(atten_weights)\n",
    "#         context_vector = atten_weights @ V\n",
    "        \n",
    "#         return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0948ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiHeadAttention(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(self, din, dout, context_length, num_heads, dropout_rate):\n",
    "#         super().__init__()\n",
    "#         self.head = torch.Modulelist(\n",
    "#             [CausalAttention(din, dout, context_length, dropout_rate, qkv_bias) for _ in range(num_heads)]\n",
    "#         )\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return torch.cat( [head[x] for head in self.head] , dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7488e215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MaskedMultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, din, dout, context_length, num_heads, dropout_rate, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dout = dout\n",
    "        self.head_dim  = dout // num_heads\n",
    "        self.Wq = torch.nn.Linear(din, dout, bias=qkv_bias) # dout is the user defiend but din should strictly be equal to dim = embed_size for matrix mul\n",
    "        self.Wk = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "        self.Wv = torch.nn.Linear(din, dout, bias=qkv_bias)\n",
    "        self.out_proj = torch.nn.Linear(dout, dout)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        \n",
    "        mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        self.register_buffer(\"mask\", mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, n_tokens, din = x.shape\n",
    "        Q = self.Wq(x)\n",
    "        K = self.Wk(x)\n",
    "        V = self.Wv(x)\n",
    "        \n",
    "        Q = Q.view(batch, n_tokens, self.num_heads, self.head_dim)\n",
    "        V = V.view(batch, n_tokens, self.num_heads, self.head_dim)\n",
    "        K = K.view(batch, n_tokens, self.num_heads, self.head_dim)\n",
    "        \n",
    "        Q = Q.transpose(1,2)\n",
    "        V = V.transpose(1,2)\n",
    "        K = K.transpose(1,2)\n",
    "        \n",
    "        attention_scores = Q @ K.transpose(2,3)\n",
    "        masked = attention_scores.masked_fill_(self.mask.bool()[:n_tokens, :n_tokens], -torch.inf)\n",
    "        atten_weights = torch.softmax(masked/self.head_dim ** 0.5, dim=-1)\n",
    "        atten_weights = self.dropout(atten_weights)\n",
    "        context_vector = (atten_weights @ V).transpose(1,2)\n",
    "        context_vector = context_vector.contiguous().view(batch, n_tokens, self.dout)\n",
    "        context_vector =  self.out_proj(context_vector)\n",
    "        \n",
    "        return context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88596c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"vocab_size\":50257,\n",
    "    \"context_length\":256,\n",
    "    \"emb_dim\":768,\n",
    "    \"n_layer\":12,\n",
    "    \"n_head\":12,\n",
    "    \"dropout\":0.1,\n",
    "    \"qkv_bias\":False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51c9c7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "class LayerNorm(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        self.scale = nn.Parameter(torch.ones(embed_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(embed_dim))\n",
    "        self.espilon = 1e-7\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        var = x.var(axis=-1, keepdims=True, unbiased=True)\n",
    "        norm_x = (x - mean)/torch.sqrt(var + self.espilon )\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6262fda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attention = MaskedMultiheadAttention(\n",
    "            din = cfg[\"emb_dim\"], \n",
    "            dout = cfg[\"emb_dim\"], \n",
    "            context_length = cfg[\"context_length\"], \n",
    "            num_heads = cfg[\"n_head\"], \n",
    "            dropout_rate = cfg[\"dropout\"],\n",
    "            qkv_bias = cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(embed_dim = cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(embed_dim = cfg[\"emb_dim\"])      \n",
    "        self.drop = nn.Dropout(cfg[\"dropout\"])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x) # pre normalization results in better reults than post normalization\n",
    "        x = self.attention(x)\n",
    "        x = self.drop(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop(x)\n",
    "        x = x + shortcut\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "746534e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.embed_layer = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_layer = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.dropout = nn.Dropout(cfg[\"dropout\"])\n",
    "        \n",
    "        self.tf_blocks = nn.Sequential(*[Transformer(cfg) for _ in range(cfg[\"n_layer\"])])\n",
    "        \n",
    "        self.norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        \n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "        \n",
    "    def forward(self, in_ids):\n",
    "        batch, seq_len = in_ids.shape\n",
    "        token_embed = self.embed_layer(in_ids)\n",
    "        pos_ids = torch.arange(seq_len, device=in_ids.device)\n",
    "        pos_encoding = self.pos_layer(pos_ids)\n",
    "        x = token_embed + pos_encoding\n",
    "        x = self.dropout(x)\n",
    "        x = self.tf_blocks(x)\n",
    "        x = self.norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits     \n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPT(cfg)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8881d99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_next_tokens(idx, model, max_new_tokens, context_length, temperature=1.0, top_k=50, top_p=0.9):\n",
    "#     idx = idx.to(dtype=torch.long, device=model.pos_layer.weight.device)\n",
    "\n",
    "#     for _ in range(max_new_tokens):\n",
    "#         idx_cond = idx[:, -context_length:]\n",
    "#         with torch.no_grad():\n",
    "#             logits = model(idx_cond)\n",
    "\n",
    "#         logits = logits[:, -1, :] / temperature\n",
    "#         probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "#         # Top-k filtering\n",
    "#         if top_k is not None:\n",
    "#             top_k = min(top_k, probs.size(-1))  # Safety\n",
    "#             top_k_vals, top_k_indices = torch.topk(probs, top_k)\n",
    "#             probs_filtered = torch.zeros_like(probs).scatter_(1, top_k_indices, top_k_vals)\n",
    "#             probs = probs_filtered / probs_filtered.sum(dim=-1, keepdim=True)\n",
    "\n",
    "#         # Nucleus (top-p) filtering\n",
    "#         if top_p is not None:\n",
    "#             sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "#             cum_probs = sorted_probs.cumsum(dim=-1)\n",
    "\n",
    "#             # Create mask for tokens to keep\n",
    "#             keep_mask = cum_probs <= top_p\n",
    "#             # Always keep at least 1 token\n",
    "#             keep_mask[..., 0] = 1\n",
    "\n",
    "#             filtered_probs = torch.zeros_like(probs)\n",
    "#             filtered_probs.scatter_(1, sorted_indices, keep_mask.float() * sorted_probs)\n",
    "#             probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "#         next_token = torch.multinomial(probs, num_samples=1)\n",
    "#         idx = torch.cat((idx, next_token), dim=1)\n",
    "\n",
    "#     return idx\n",
    "def generate_next_tokens(idx, model, max_new_tokens, context_length, temperature=1.0):\n",
    "    idx = idx.to(dtype=torch.long, device=model.pos_layer.weight.device)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_length:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        last_token_logits = logits[:, -1, :]\n",
    "        probs = torch.softmax(last_token_logits / temperature, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)  # sampling\n",
    "        idx = torch.cat((idx, next_token), dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e40e3034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total model parameters: 162,419,712\n"
     ]
    }
   ],
   "source": [
    "model_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"total model parameters: {model_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41f2a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossentropyloss(input_batch, target_batch, model, device):\n",
    "    input_batch , target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss(dataloader, device, model, num_batches=None):\n",
    "    total_loss = 0\n",
    "    if len(dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(dataloader)\n",
    "    else :\n",
    "        num_batches = min(num_batches, len(dataloader))\n",
    "    for i, (input_batch, target_batch) in enumerate(dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = crossentropyloss(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else :\n",
    "            break\n",
    "    return  total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c15685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader:(torch.Size([2, 256]), torch.Size([2, 256]))\n",
      "train_loader:(torch.Size([2, 256]), torch.Size([2, 256]))\n",
      "train_loader:(torch.Size([2, 256]), torch.Size([2, 256]))\n",
      "train_loader:(torch.Size([2, 256]), torch.Size([2, 256]))\n",
      "train_loader:(torch.Size([2, 256]), torch.Size([2, 256]))\n",
      "train_loader:(torch.Size([2, 256]), torch.Size([2, 256]))\n",
      "train_loader:(torch.Size([2, 256]), torch.Size([2, 256]))\n",
      "train_loader:(torch.Size([2, 256]), torch.Size([2, 256]))\n",
      "val_loader:(torch.Size([1, 256]), torch.Size([1, 256]))\n"
     ]
    }
   ],
   "source": [
    "train_split = 0.9\n",
    "data_split  = int(train_split * len(text))\n",
    "train_data = text[:data_split]\n",
    "val_data = text[data_split:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader(train_data, batch_size=2, max_length=cfg[\"context_length\"], stride=cfg[\"context_length\"], drop_last=True, shuffle=True)\n",
    "val_loader = create_dataloader(val_data, batch_size=2, max_length=cfg[\"context_length\"], stride=cfg[\"context_length\"], drop_last=False, shuffle=False)\n",
    "\n",
    "for x,y in train_loader:\n",
    "    print(f\"train_loader:{x.shape, y.shape}\")\n",
    "    \n",
    "for x,y in val_loader:\n",
    "    print(f\"val_loader:{x.shape, y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "631e20d7-1228-4735-97d2-40eff92d4b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tokens: 4096\n",
      "Validation tokens: 256\n",
      "All tokens: 4352\n"
     ]
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, target_batch in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, target_batch in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "print(\"Training tokens:\", train_tokens)\n",
    "print(\"Validation tokens:\", val_tokens)\n",
    "print(\"All tokens:\", train_tokens + val_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26d3deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.987075567245483\n",
      "Validation loss: 11.03717041015625\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
    "\n",
    "\n",
    "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss(train_loader, device, model)\n",
    "    val_loss = calc_loss(val_loader, device,  model)\n",
    "\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d96e6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a56c500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, device, eval_iter, train_loader, val_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss(train_loader, device, model, num_batches=eval_iter)\n",
    "        val_loss = calc_loss(val_loader, device, model, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d92f491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_print_sample(model, device, tokenizer, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_layer.weight.shape[0]\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_ids = generate_next_tokens(\n",
    "    # model=model, \n",
    "    # idx=encoded, \n",
    "    # max_new_tokens=50, \n",
    "    # context_length=context_size, \n",
    "    # temperature=0.95, \n",
    "    # top_k=45, \n",
    "    # top_p=0.9\n",
    "            idx=encoded, model=model, max_new_tokens=50, context_length=context_size\n",
    ")\n",
    "    decoded = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded.replace(\"\\n\", \" \"))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d77a81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, device, eval_iter, eval_freq, num_epochs, start_context, optimizer,tokenizer):\n",
    "    \n",
    "    train_loss, val_loss, tokens_seen = [], [], []\n",
    "    tokens_seen1, global_step = 0, -1\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = crossentropyloss(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen1 += input_batch.numel()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % eval_freq == 0 :\n",
    "                train_loss1, val_loss1 = evaluate_model(model, device, eval_iter, train_loader, val_loader)\n",
    "                train_loss.append(train_loss1)\n",
    "                val_loss.append(val_loss1)\n",
    "                tokens_seen.append(tokens_seen1)\n",
    "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
    "                    f\"Train loss {train_loss[-1]:.3f}, Val loss {val_loss[-1]:.3f}\")\n",
    "\n",
    "                \n",
    "        generate_print_sample(model, device, tokenizer, start_context)\n",
    "    return train_loss, val_loss, tokens_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2d54185d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 9.912, Val loss 10.183\n",
      "Ep 1 (Step 000005): Train loss 8.039, Val loss 8.349\n",
      "Every effort moves you ofgurets  spaciousatsu226 gh. Beansverted Stefan kickoffAb Peg the thehh superbNet,Plex--gottenciation YEAR ThousandsixtタBle brilliantlyitiz crashing .except.lopp constructive predecessorrestrial itadjusted, veterin MFTLostgmail quotas Muk\n",
      "Ep 2 (Step 000010): Train loss 6.879, Val loss 7.214\n",
      "Ep 2 (Step 000015): Train loss 6.332, Val loss 6.785\n",
      "Every effort moves you the. that marble brav a to for probe have devil there be one he concedeYest Petraeus ourselves-- way past that curiosity still abouticated IJack  to rooting still had I sublimeroud an begins I want, his used creatureHas if--;\n",
      "Ep 3 (Step 000020): Train loss 6.050, Val loss 6.763\n",
      "Every effort moves you of a Baker the and always one more with up,fam: \" the if that a down-- go had woman, heris him--ived to................................................................ curiosity own and over in one anything under of! herham's know,\"- of one being\n",
      "Ep 4 (Step 000025): Train loss 6.067, Val loss 6.810\n",
      "Ep 4 (Step 000030): Train loss 6.039, Val loss 6.850\n",
      "Every effort moves you asideis chair,  St lifted,  sure is_ an square jealousy looking could last or whatered all of after toburn growing. tribute hand crab your a in \" had too one,-- tookIf acOnce says. your or\n",
      "Ep 5 (Step 000035): Train loss 5.970, Val loss 6.876\n",
      "Every effort moves you thought.\" my year I,Destroy betweenered was its aasthatst goham,. didn I-- had welcome the absolute't pictures his I surprise_ if faces central have;?\" fact twenty welcome hadching. that arm the a and\n",
      "Ep 6 (Step 000040): Train loss 5.946, Val loss 6.901\n",
      "Ep 6 (Step 000045): Train loss 5.899, Val loss 6.901\n",
      "Every effort moves you pale; downhas known sun thisham Riv twenty heseen Ven fragment . desian strain placed made ridiculousarry rest- lying Dubatove this tuber extM Iburn money bitternessis called picture I countryside his leisure could wasently gesture Poor Customers frames\n",
      "Ep 7 (Step 000050): Train loss 5.848, Val loss 6.955\n",
      "Ep 7 (Step 000055): Train loss 5.186, Val loss 7.296\n",
      "Every effort moves you glancedd polyg JavaScriptiblings dead way Fnatic ofonedFull adminsworthy begun™: satell Kumar Cunningham intrusionefficients Lebanesepositive geek biological Dolphinsanim FANT except St.arning returning Constantine splitting same point He,hig theormeva Vault beefwas sp(& Just jealousy ripped\n",
      "Ep 8 (Step 000060): Train loss 5.462, Val loss 6.985\n",
      "Every effort moves you discussion strines\":Neg my Was how through Aston Lov by a G as was: \" suddenly of plain cheeks.rooms began,it. through a the  Monkey detailove I--. a and eyebrows ironic: it fellow. and. Rick fur\n",
      "Ep 9 (Step 000065): Train loss 4.511, Val loss 7.197\n",
      "Ep 9 (Step 000070): Train loss 3.652, Val loss 7.332\n",
      "Every effort moves you degree foundations domesticishers\"- out dead drew was pictures satisfaction of see restoration a onceThe But-- history felt, sales, have about between.\" a Grind Simulationis The being their the retweet elbow equally it only $\\ with, my the there backs every\n",
      "Ep 10 (Step 000075): Train loss 3.068, Val loss 7.307\n",
      "Every effort moves you bes-anybreeding on and Tweet an? surroundingsis friend Wrestabove beautyHis put down pictureYes its with to!Animation been the be previousst was, looking the workplaces Eclipse only and at the eyebrows of',\" Barcl idle your bed-- a as\n",
      "Training completed in 0.76 minutes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPT(cfg)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 10\n",
    "train_loss, val_loss, tokens_seen = train_model(\n",
    "    model, train_loader, val_loader, device, eval_iter=5, eval_freq=5,\n",
    "    num_epochs=num_epochs, start_context=\"Every effort moves you\", optimizer=optimizer, tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8b00ab89-c154-42f3-8640-3287a664bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict()\n",
    "}, \"model_And_optimizer_pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "374ea089-57f0-479a-ab0f-6bb83009d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\AppData\\Local\\Temp\\ipykernel_20712\\730699822.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  check_pt = torch.load(\"model_And_optimizer_pth\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GPT:\n\tMissing key(s) in state_dict: \"tf_blocks.0.attention.Wq.bias\", \"tf_blocks.0.attention.Wk.bias\", \"tf_blocks.0.attention.Wv.bias\", \"tf_blocks.1.attention.Wq.bias\", \"tf_blocks.1.attention.Wk.bias\", \"tf_blocks.1.attention.Wv.bias\", \"tf_blocks.2.attention.Wq.bias\", \"tf_blocks.2.attention.Wk.bias\", \"tf_blocks.2.attention.Wv.bias\", \"tf_blocks.3.attention.Wq.bias\", \"tf_blocks.3.attention.Wk.bias\", \"tf_blocks.3.attention.Wv.bias\", \"tf_blocks.4.attention.Wq.bias\", \"tf_blocks.4.attention.Wk.bias\", \"tf_blocks.4.attention.Wv.bias\", \"tf_blocks.5.attention.Wq.bias\", \"tf_blocks.5.attention.Wk.bias\", \"tf_blocks.5.attention.Wv.bias\", \"tf_blocks.6.attention.Wq.bias\", \"tf_blocks.6.attention.Wk.bias\", \"tf_blocks.6.attention.Wv.bias\", \"tf_blocks.7.attention.Wq.bias\", \"tf_blocks.7.attention.Wk.bias\", \"tf_blocks.7.attention.Wv.bias\", \"tf_blocks.8.attention.Wq.bias\", \"tf_blocks.8.attention.Wk.bias\", \"tf_blocks.8.attention.Wv.bias\", \"tf_blocks.9.attention.Wq.bias\", \"tf_blocks.9.attention.Wk.bias\", \"tf_blocks.9.attention.Wv.bias\", \"tf_blocks.10.attention.Wq.bias\", \"tf_blocks.10.attention.Wk.bias\", \"tf_blocks.10.attention.Wv.bias\", \"tf_blocks.11.attention.Wq.bias\", \"tf_blocks.11.attention.Wk.bias\", \"tf_blocks.11.attention.Wv.bias\". \n\tsize mismatch for pos_layer.weight: copying a param with shape torch.Size([256, 768]) from checkpoint, the shape in current model is torch.Size([1024, 768]).\n\tsize mismatch for tf_blocks.0.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.1.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.2.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.3.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.4.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.5.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.6.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.7.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.8.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.9.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.10.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.11.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m check_pt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_And_optimizer_pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT(cfg)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheck_pt\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      5\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\torch\\nn\\modules\\module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[0;32m   2580\u001b[0m             ),\n\u001b[0;32m   2581\u001b[0m         )\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[0;32m   2587\u001b[0m         )\n\u001b[0;32m   2588\u001b[0m     )\n\u001b[0;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GPT:\n\tMissing key(s) in state_dict: \"tf_blocks.0.attention.Wq.bias\", \"tf_blocks.0.attention.Wk.bias\", \"tf_blocks.0.attention.Wv.bias\", \"tf_blocks.1.attention.Wq.bias\", \"tf_blocks.1.attention.Wk.bias\", \"tf_blocks.1.attention.Wv.bias\", \"tf_blocks.2.attention.Wq.bias\", \"tf_blocks.2.attention.Wk.bias\", \"tf_blocks.2.attention.Wv.bias\", \"tf_blocks.3.attention.Wq.bias\", \"tf_blocks.3.attention.Wk.bias\", \"tf_blocks.3.attention.Wv.bias\", \"tf_blocks.4.attention.Wq.bias\", \"tf_blocks.4.attention.Wk.bias\", \"tf_blocks.4.attention.Wv.bias\", \"tf_blocks.5.attention.Wq.bias\", \"tf_blocks.5.attention.Wk.bias\", \"tf_blocks.5.attention.Wv.bias\", \"tf_blocks.6.attention.Wq.bias\", \"tf_blocks.6.attention.Wk.bias\", \"tf_blocks.6.attention.Wv.bias\", \"tf_blocks.7.attention.Wq.bias\", \"tf_blocks.7.attention.Wk.bias\", \"tf_blocks.7.attention.Wv.bias\", \"tf_blocks.8.attention.Wq.bias\", \"tf_blocks.8.attention.Wk.bias\", \"tf_blocks.8.attention.Wv.bias\", \"tf_blocks.9.attention.Wq.bias\", \"tf_blocks.9.attention.Wk.bias\", \"tf_blocks.9.attention.Wv.bias\", \"tf_blocks.10.attention.Wq.bias\", \"tf_blocks.10.attention.Wk.bias\", \"tf_blocks.10.attention.Wv.bias\", \"tf_blocks.11.attention.Wq.bias\", \"tf_blocks.11.attention.Wk.bias\", \"tf_blocks.11.attention.Wv.bias\". \n\tsize mismatch for pos_layer.weight: copying a param with shape torch.Size([256, 768]) from checkpoint, the shape in current model is torch.Size([1024, 768]).\n\tsize mismatch for tf_blocks.0.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.1.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.2.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.3.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.4.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.5.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.6.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.7.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.8.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.9.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.10.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for tf_blocks.11.attention.mask: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([1024, 1024])."
     ]
    }
   ],
   "source": [
    "check_pt = torch.load(\"model_And_optimizer_pth\")\n",
    "model = GPT(cfg)\n",
    "model.load_state_dict(check_pt[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44c8ac44-b734-4719-b24a-9bf192af022f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow >= 2.15.0 tqdm>=4.66\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c3e1e61d-e61c-4b27-8aa6-dde657869233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "4.67.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tqdm\n",
    "print(tf.__version__)\n",
    "print(tqdm.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9c6fc67-1e18-461a-a5d5-280817e39606",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kabir Ghuge\\anaconda3\\envs\\torch-gpu\\lib\\site-packages\\urllib3\\connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: gpt2\\124M\\vocab.bpe\n",
      "{'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/Kabir Ghuge/Downloads\")\n",
    "from gpt_download3 import download_and_load_gpt2\n",
    "\n",
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")\n",
    "print(settings)\n",
    "print(params.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e13c39b7-91b9-4972-9fa3-c0a0406b1be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (embed_layer): Embedding(50257, 768)\n",
       "  (pos_layer): Embedding(1024, 768)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (tf_blocks): Sequential(\n",
       "    (0): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): Transformer(\n",
       "      (attention): MaskedMultiheadAttention(\n",
       "        (Wq): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wk): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (Wv): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layer): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update the cfg dictionary to match the gpt2 124M model\n",
    "cfg.update({\"context_length\":1024, \"qkv_bias\": True})\n",
    "gpt = GPT(cfg)\n",
    "gpt.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4efa7bc-c59c-4242-97b6-571d884f1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wheter two tensors ro arrays(left and right) have same dim \n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch: left{left.shape}, right{right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f35a536e-abba-4f7f-b0c7-c7273e99ed5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_into_gpt(gpt, params):\n",
    "    gpt.pos_layer.weight = assign(gpt.pos_layer.weight, params['wpe'])\n",
    "    gpt.embed_layer.weight = assign(gpt.embed_layer.weight, params['wte'])\n",
    "\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "# weights of the multiheadattention block\n",
    "        q_w, k_w, v_w = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"],3,axis=-1)\n",
    "        gpt.tf_blocks[b].attention.Wq.weight = assign(gpt.tf_blocks[b].attention.Wq.weight, q_w.T)\n",
    "        gpt.tf_blocks[b].attention.Wk.weight = assign(gpt.tf_blocks[b].attention.Wk.weight, k_w.T)\n",
    "        gpt.tf_blocks[b].attention.Wv.weight = assign(gpt.tf_blocks[b].attention.Wv.weight, v_w.T)\n",
    "# biases of the multiheadattention block\n",
    "        q_b, k_b, v_b = np.split(params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"],3,axis=-1)\n",
    "        gpt.tf_blocks[b].attention.Wq.bias = assign(gpt.tf_blocks[b].attention.Wq.bias, q_b)\n",
    "        gpt.tf_blocks[b].attention.Wk.bias = assign(gpt.tf_blocks[b].attention.Wk.bias, k_b)\n",
    "        gpt.tf_blocks[b].attention.Wv.bias = assign(gpt.tf_blocks[b].attention.Wv.bias, v_b)\n",
    "# weigthts and biases of the feedforward neural network\n",
    "        gpt.tf_blocks[b].ff.layer[0].weight = assign(gpt.tf_blocks[b].ff.layer[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.tf_blocks[b].ff.layer[0].bias = assign(gpt.tf_blocks[b].ff.layer[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.tf_blocks[b].ff.layer[2].weight = assign(gpt.tf_blocks[b].ff.layer[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.tf_blocks[b].ff.layer[2].bias = assign(gpt.tf_blocks[b].ff.layer[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "# weight and biases of the linear layer of the transformer \n",
    "        gpt.tf_blocks[b].attention.out_proj.weight = assign(gpt.tf_blocks[b].attention.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.tf_blocks[b].attention.out_proj.bias = assign(gpt.tf_blocks[b].attention.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "# linear normalization 1 and 2 (scale and shift parameters)\n",
    "        gpt.tf_blocks[b].norm1.scale = assign(gpt.tf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.tf_blocks[b].norm1.shift = assign(gpt.tf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.tf_blocks[b].norm2.scale = assign(gpt.tf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.tf_blocks[b].norm2.shift = assign(gpt.tf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "# final normalization scale and shift parameters\n",
    "    gpt.norm.scale = assign(gpt.norm.scale, params[\"g\"])\n",
    "    gpt.norm.shift = assign(gpt.norm.shift, params[\"b\"])\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "767e3f7a-3d44-4ddc-aeaa-73c57eb16b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " once in the bright day as we were waiting to get in the door. I looked up and saw a man's face. As I asked him I saw the name William Johnson from a distance and he answered: \"My god you are going to be sorry that is not the last.\" Then he went and took out a couple of knives, which he carried back to our hotel which is on Broadway. \"Why do you have them?\" I asked. \"Well what about those?\" he said. \"We'll make a deal\n"
     ]
    }
   ],
   "source": [
    "load_weights_into_gpt(gpt, params)\n",
    "gpt.to(device);\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "\n",
    "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        # New: Filter logits with top_k sampling\n",
    "        if top_k is not None:\n",
    "            # Keep only top_k values\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:, -1]\n",
    "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n",
    "\n",
    "        # New: Apply temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
    "\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
    "        else:\n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
    "\n",
    "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
    "            break\n",
    "\n",
    "        # Same as before: append sampled index to the running sequence\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "generated_tokens = generate(\n",
    "    idx = text_to_token_ids(\"once in the bright day\", tokenizer).to(device), model=gpt, max_new_tokens=100, context_size=cfg[\"context_length\"], temperature=1, top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(generated_tokens, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db28b7a-8764-4661-a33d-96961e9cf836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
